{"cells":[{"cell_type":"markdown","id":"7400f5d7","metadata":{},"source":["# Text Sentiment Classification By LSTM"]},{"cell_type":"code","execution_count":63,"id":"76eca214","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:35:54.024828Z","iopub.status.busy":"2023-03-24T04:35:54.024206Z","iopub.status.idle":"2023-03-24T04:35:54.031204Z","shell.execute_reply":"2023-03-24T04:35:54.029928Z","shell.execute_reply.started":"2023-03-24T04:35:54.024772Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","path_prefix = './'"]},{"cell_type":"markdown","id":"4d7ab69a","metadata":{},"source":["## Data Loading"]},{"cell_type":"code","execution_count":64,"id":"48137877","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:35:54.034296Z","iopub.status.busy":"2023-03-24T04:35:54.033776Z","iopub.status.idle":"2023-03-24T04:35:54.049775Z","shell.execute_reply":"2023-03-24T04:35:54.048753Z","shell.execute_reply.started":"2023-03-24T04:35:54.034258Z"},"trusted":true},"outputs":[],"source":["import torch\n","import numpy as np\n","import pandas as pd\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import ConcatDataset, DataLoader, Subset\n","\n","def load_training_data(path='Train_label.txt'):\n","    # Read training data\n","    if 'Train_label' in path:\n","        with open(path, 'r') as f:\n","            lines = f.readlines()\n","            lines = [line.strip('\\n').split(' ') for line in lines]\n","        x = [line[2:] for line in lines]\n","        y = [line[0] for line in lines]\n","        return x, y\n","    else:\n","        with open(path, 'r') as f:\n","            lines = f.readlines()\n","            x = [line.strip('\\n').split(' ') for line in lines]\n","        return x\n","\n","def load_testing_data(path='Test.txt'):\n","    # Read testing data\n","    with open(path, 'r') as f:\n","        lines = f.readlines()\n","        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\n","        X = [sen.split(' ') for sen in X]\n","    return X\n","\n","def evaluation(outputs, labels):\n","    #outputs => probability (float)\n","    #labels => labels\n","    outputs[outputs>=0.5] = 1 # Negtive Sentiment\n","    outputs[outputs<0.5] = 0 # Positive Sentiment\n","    correct = torch.sum(torch.eq(outputs, labels)).item()\n","    return correct"]},{"cell_type":"code","execution_count":65,"id":"40c56fda","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:35:54.051813Z","iopub.status.busy":"2023-03-24T04:35:54.051387Z","iopub.status.idle":"2023-03-24T04:35:54.073820Z","shell.execute_reply":"2023-03-24T04:35:54.070890Z","shell.execute_reply.started":"2023-03-24T04:35:54.051777Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(0)\n","if torch.cuda.is_available():\n","    torch.cuda.manual_seed(0)\n","    torch.cuda.manual_seed_all(0)  \n","np.random.seed(0)  \n","torch.backends.cudnn.benchmark = False\n","torch.backends.cudnn.deterministic = True\n"]},{"cell_type":"markdown","id":"cb4ab257","metadata":{},"source":["## Train Word to Vector"]},{"cell_type":"code","execution_count":66,"id":"6c01f0c9","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:35:54.077860Z","iopub.status.busy":"2023-03-24T04:35:54.077144Z","iopub.status.idle":"2023-03-24T04:36:35.114593Z","shell.execute_reply":"2023-03-24T04:36:35.113493Z","shell.execute_reply.started":"2023-03-24T04:35:54.077823Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading training data ...\n","loading testing data ...\n","saving model ...\n"]}],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import argparse\n","from gensim.models import word2vec\n","\n","def train_word2vec(x):\n","    model = word2vec.Word2Vec(x, vector_size=250, window=5, min_count=5, workers=12)\n","    return model\n","\n","if __name__ == \"__main__\":\n","    print(\"loading training data ...\")\n","    train_x, y = load_training_data('Train_label.txt')\n","    train_x_no_label = load_training_data('Train_nolabel.txt'))\n","\n","    print(\"loading testing data ...\")\n","    test_x = load_testing_data('Test.txt')\n","\n","    model = train_word2vec(train_x + train_x_no_label + test_x)\n","    \n","    print(\"saving model ...\")\n","    model.save(os.path.join(path_prefix, 'w2v_all.model'))"]},{"cell_type":"markdown","id":"bd85688b","metadata":{},"source":["## Data Preprocess\n"]},{"cell_type":"code","execution_count":67,"id":"21740a98","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:36:35.116462Z","iopub.status.busy":"2023-03-24T04:36:35.116143Z","iopub.status.idle":"2023-03-24T04:36:35.132561Z","shell.execute_reply":"2023-03-24T04:36:35.131501Z","shell.execute_reply.started":"2023-03-24T04:36:35.116434Z"},"trusted":true},"outputs":[],"source":["from torch import nn\n","from gensim.models import Word2Vec\n","\n","class Preprocess():\n","    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\n","        self.w2v_path = w2v_path\n","        self.sentences = sentences\n","        self.sen_len = sen_len\n","        self.idx2word = []\n","        self.word2idx = {}\n","        self.embedding_matrix = []\n","    def get_w2v_model(self):\n","        # load word to vector model\n","        self.embedding = Word2Vec.load(self.w2v_path)\n","        self.embedding_dim = self.embedding.vector_size\n","    def add_embedding(self, word):\n","        # add word into embedding\n","        vector = torch.empty(1, self.embedding_dim)\n","        torch.nn.init.uniform_(vector)\n","        self.word2idx[word] = len(self.word2idx)\n","        self.idx2word.append(word)\n","        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n","    def make_embedding(self, load=True):\n","        print(\"Get embedding ...\")\n","        if load:\n","            print(\"loading word to vec model ...\")\n","            self.get_w2v_model()\n","        else:\n","            raise NotImplementedError\n","\n","        for i, word in enumerate(self.embedding.wv.key_to_index):\n","            print('get words #{}'.format(i+1), end='\\r')\n","            self.word2idx[word] = len(self.word2idx)\n","            self.idx2word.append(word)\n","            self.embedding_matrix.append(self.embedding.wv[word])\n","        print('')\n","        self.embedding_matrix = torch.tensor(self.embedding_matrix)\n","        self.add_embedding(\"<PAD>\")\n","        self.add_embedding(\"<UNK>\")\n","        print(\"total words: {}\".format(len(self.embedding_matrix)))\n","        return self.embedding_matrix\n","    def pad_sequence(self, sentence):\n","        if len(sentence) > self.sen_len:\n","            sentence = sentence[:self.sen_len]\n","        else:\n","            pad_len = self.sen_len - len(sentence)\n","            for _ in range(pad_len):\n","                sentence.append(self.word2idx[\"<PAD>\"])\n","        assert len(sentence) == self.sen_len\n","        return sentence\n","    def sentence_word2idx(self):\n","        sentence_list = []\n","        for i, sen in enumerate(self.sentences):\n","            print('sentence count #{}'.format(i+1), end='\\r')\n","            sentence_idx = []\n","            for word in sen:\n","                if (word in self.word2idx.keys()):\n","                    sentence_idx.append(self.word2idx[word])\n","                else:\n","                    sentence_idx.append(self.word2idx[\"<UNK>\"])\n","            sentence_idx = self.pad_sequence(sentence_idx)\n","            sentence_list.append(sentence_idx)\n","        return torch.LongTensor(sentence_list)\n","    def labels_to_tensor(self, y):\n","        # turn labels into tensors\n","        y = [int(label) for label in y]\n","        return torch.LongTensor(y)"]},{"cell_type":"markdown","id":"4c33fbe7","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":68,"id":"ca402108","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:36:35.134660Z","iopub.status.busy":"2023-03-24T04:36:35.133832Z","iopub.status.idle":"2023-03-24T04:36:35.148360Z","shell.execute_reply":"2023-03-24T04:36:35.147336Z","shell.execute_reply.started":"2023-03-24T04:36:35.134618Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch.utils import data\n","\n","class TwitterDataset(data.Dataset):\n","    \"\"\"\n","    Expected data shape like:(data_num, data_len)\n","    Data can be a list of numpy array or a list of lists\n","    input data shape : (data_num, seq_len, feature_dim)\n","    \n","    __len__ will return the number of data\n","    \"\"\"\n","    def __init__(self, X, y):\n","        self.data = X\n","        self.label = y\n","    def __getitem__(self, idx):\n","        if self.label is None: return self.data[idx]\n","        return self.data[idx], self.label[idx]\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"markdown","id":"3a85319a","metadata":{},"source":["## LSTM Model"]},{"cell_type":"code","execution_count":69,"id":"9531cf7e","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:36:35.151444Z","iopub.status.busy":"2023-03-24T04:36:35.151011Z","iopub.status.idle":"2023-03-24T04:36:35.164233Z","shell.execute_reply":"2023-03-24T04:36:35.163290Z","shell.execute_reply.started":"2023-03-24T04:36:35.151409Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","class LSTM_Net(nn.Module):\n","    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\n","        super(LSTM_Net, self).__init__()\n","        # embedding layer\n","        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\n","        self.embedding.weight = torch.nn.Parameter(embedding)\n","        # Whether fix embedding\n","        self.embedding.weight.requires_grad = False if fix_embedding else True\n","        self.embedding_dim = embedding.size(1)\n","        self.hidden_dim = hidden_dim\n","        self.num_layers = num_layers\n","        self.dropout = dropout\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n","        self.classifier = nn.Sequential( nn.Dropout(dropout),\n","                                         nn.Linear(hidden_dim, 1))\n","                                         #nn.Tanh())\n","                                         #nn.Sigmoid())\n","                                        #nn.ReLU())\n","    def forward(self, inputs):\n","        inputs = self.embedding(inputs)\n","        x, _ = self.lstm(inputs, None)\n","        # dimension of x (batch, seq_len, hidden_size)\n","        x = x[:, -1, :] \n","        x = self.classifier(x)\n","        return x"]},{"cell_type":"markdown","id":"f0045a0c","metadata":{},"source":["## Define Training"]},{"cell_type":"code","execution_count":70,"id":"2176479d","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:36:35.166124Z","iopub.status.busy":"2023-03-24T04:36:35.165727Z","iopub.status.idle":"2023-03-24T04:36:35.184154Z","shell.execute_reply":"2023-03-24T04:36:35.183123Z","shell.execute_reply.started":"2023-03-24T04:36:35.166057Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\n","    total = sum(p.numel() for p in model.parameters())\n","    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\n","    model.train() # set training mode\n","    #criterion = nn.BCELoss() # Define loss function\n","    criterion = nn.BCEWithLogitsLoss()\n","    t_batch = len(train) \n","    v_batch = len(valid) \n","    do_semi = False \n","    #optimizer = optim.SGD(model.parameters(), lr=lr) # set optimizer as SGD (you can change it)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=1e-4)\n","    total_loss, total_acc, best_acc = 0, 0, 0\n","    for epoch in range(n_epoch):\n","        total_loss, total_acc = 0, 0\n","        \n","        \n","        # For training\n","        for i, (inputs, labels) in enumerate(train):\n","            inputs = inputs.to(device, dtype=torch.long) # set device \"cuda\"\n","            labels = labels.to(device, dtype=torch.float) # set device \"cuda\"\n","            optimizer.zero_grad()\n","            outputs = model(inputs)\n","            outputs = outputs.squeeze()\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            correct = evaluation(outputs, labels) # calculate accuracy\n","            total_acc += (correct / batch_size)\n","            total_loss += loss.item()\n","            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\n","            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\n","        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\n","\n","        # For validation\n","        model.eval() # set validation mode\n","        with torch.no_grad():\n","            total_loss, total_acc = 0, 0\n","            for i, (inputs, labels) in enumerate(valid):\n","                inputs = inputs.to(device, dtype=torch.long) # set device \"cuda\"\n","                labels = labels.to(device, dtype=torch.float) # set device \"cuda\"\n","                outputs = model(inputs)\n","                outputs = outputs.squeeze()\n","                loss = criterion(outputs, labels)\n","                correct = evaluation(outputs, labels)\n","                total_acc += (correct / batch_size)\n","                total_loss += loss.item()\n","\n","            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\n","            if total_acc > best_acc:\n","                # if the result of validation is better than previous model, save the new model\n","                best_acc = total_acc\n","                torch.save(model, \"{}/ckpt.model\".format(model_dir))\n","                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\n","        print('-----------------------------------------------')\n","        model.train()"]},{"cell_type":"markdown","id":"5f5b610f","metadata":{},"source":["## Testing"]},{"cell_type":"code","execution_count":71,"id":"5a3d37fc","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:36:35.185632Z","iopub.status.busy":"2023-03-24T04:36:35.185214Z","iopub.status.idle":"2023-03-24T04:36:35.202082Z","shell.execute_reply":"2023-03-24T04:36:35.201121Z","shell.execute_reply.started":"2023-03-24T04:36:35.185597Z"},"trusted":true},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","def testing(batch_size, test_loader, model, device):\n","    model.eval()\n","    ret_output = []\n","    with torch.no_grad():\n","        for i, inputs in enumerate(test_loader):\n","            inputs = inputs.to(device, dtype=torch.long)\n","            outputs = model(inputs)\n","            outputs = outputs.squeeze()\n","            outputs[outputs>=0.5] = 1\n","            outputs[outputs<0.5] = 0\n","            ret_output += outputs.int().tolist()\n","    \n","    return ret_output"]},{"cell_type":"code","execution_count":null,"id":"9e212b81","metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","id":"0b82c279","metadata":{},"source":["## Parameter setting + Train"]},{"cell_type":"code","execution_count":72,"id":"fef84568","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:36:35.205524Z","iopub.status.busy":"2023-03-24T04:36:35.204568Z","iopub.status.idle":"2023-03-24T04:45:14.370036Z","shell.execute_reply":"2023-03-24T04:45:14.368714Z","shell.execute_reply.started":"2023-03-24T04:36:35.205483Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading data ...\n","Get embedding ...\n","loading word to vec model ...\n","get words #23000\n","total words: 23002\n","sentence count #149918\n","start training, parameter total:6252751, trainable:502251\n","\n","[ Epoch1: 4063/4063 ] loss:0.375 acc:43.750 \n","Train | Loss:0.49724 Acc: 74.227\n","Valid | Loss:0.44972 Acc: 76.926 \n","saving model with acc 76.926\n","-----------------------------------------------\n","[ Epoch2: 4063/4063 ] loss:0.363 acc:40.625  \n","Train | Loss:0.44095 Acc: 78.417\n","Valid | Loss:0.43354 Acc: 78.381 \n","saving model with acc 78.381\n","-----------------------------------------------\n","[ Epoch3: 4063/4063 ] loss:0.250 acc:46.875  \n","Train | Loss:0.42412 Acc: 79.473\n","Valid | Loss:0.43346 Acc: 79.941 \n","saving model with acc 79.941\n","-----------------------------------------------\n","[ Epoch4: 4063/4063 ] loss:0.343 acc:34.375  \n","Train | Loss:0.41184 Acc: 80.276\n","Valid | Loss:0.42333 Acc: 79.058 \n","-----------------------------------------------\n","[ Epoch5: 4063/4063 ] loss:0.242 acc:43.750  \n","Train | Loss:0.40061 Acc: 80.865\n","Valid | Loss:0.43346 Acc: 80.467 \n","saving model with acc 80.467\n","-----------------------------------------------\n","[ Epoch6: 4063/4063 ] loss:0.312 acc:40.625  \n","Train | Loss:0.39144 Acc: 81.505\n","Valid | Loss:0.41830 Acc: 79.931 \n","-----------------------------------------------\n","[ Epoch7: 4063/4063 ] loss:0.513 acc:34.375  \n","Train | Loss:0.38318 Acc: 81.995\n","Valid | Loss:0.42023 Acc: 79.590 \n","-----------------------------------------------\n","[ Epoch8: 4063/4063 ] loss:0.337 acc:40.625  \n","Train | Loss:0.37537 Acc: 82.468\n","Valid | Loss:0.42339 Acc: 81.039 \n","saving model with acc 81.039\n","-----------------------------------------------\n","[ Epoch9: 4063/4063 ] loss:0.414 acc:46.875  \n","Train | Loss:0.36571 Acc: 82.997\n","Valid | Loss:0.42373 Acc: 78.326 \n","-----------------------------------------------\n","[ Epoch10: 4063/4063 ] loss:0.256 acc:43.750  \n","Train | Loss:0.35734 Acc: 83.548\n","Valid | Loss:0.43332 Acc: 80.518 \n","-----------------------------------------------\n","[ Epoch11: 4063/4063 ] loss:0.406 acc:43.750  \n","Train | Loss:0.34869 Acc: 84.005\n","Valid | Loss:0.43213 Acc: 80.498 \n","-----------------------------------------------\n","[ Epoch12: 4063/4063 ] loss:0.760 acc:31.250  \n","Train | Loss:0.33803 Acc: 84.610\n","Valid | Loss:0.44079 Acc: 79.239 \n","-----------------------------------------------\n","[ Epoch13: 4063/4063 ] loss:0.239 acc:37.500  \n","Train | Loss:0.32805 Acc: 85.098\n","Valid | Loss:0.45525 Acc: 79.690 \n","-----------------------------------------------\n","[ Epoch14: 4063/4063 ] loss:0.299 acc:43.750  \n","Train | Loss:0.31917 Acc: 85.749\n","Valid | Loss:0.44899 Acc: 79.524 \n","-----------------------------------------------\n","[ Epoch15: 4063/4063 ] loss:0.177 acc:43.750  \n","Train | Loss:0.30792 Acc: 86.328\n","Valid | Loss:0.45313 Acc: 78.948 \n","-----------------------------------------------\n","[ Epoch16: 4063/4063 ] loss:0.339 acc:40.625  \n","Train | Loss:0.29697 Acc: 86.911\n","Valid | Loss:0.46054 Acc: 78.782 \n","-----------------------------------------------\n","[ Epoch17: 4063/4063 ] loss:0.439 acc:37.500  \n","Train | Loss:0.28657 Acc: 87.427\n","Valid | Loss:0.48463 Acc: 79.143 \n","-----------------------------------------------\n","[ Epoch18: 4063/4063 ] loss:0.383 acc:43.750  \n","Train | Loss:0.27664 Acc: 87.989\n","Valid | Loss:0.48347 Acc: 79.284 \n","-----------------------------------------------\n","[ Epoch19: 4063/4063 ] loss:0.286 acc:46.875  \n","Train | Loss:0.26393 Acc: 88.704\n","Valid | Loss:0.49753 Acc: 79.208 \n","-----------------------------------------------\n","[ Epoch20: 4063/4063 ] loss:0.195 acc:43.750  \n","Train | Loss:0.25411 Acc: 89.224\n","Valid | Loss:0.51121 Acc: 78.897 \n","-----------------------------------------------\n"]}],"source":["# main.py\n","import os\n","import torch\n","import argparse\n","import numpy as np\n","from torch import nn\n","from gensim.models import word2vec\n","from sklearn.model_selection import train_test_split\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# set data path\n","train_with_label = os.path.join(path_prefix, 'Train_label.txt')\n","train_no_label = os.path.join(path_prefix, 'Train_nolabel.txt')\n","testing_data = os.path.join(path_prefix, 'Test.txt')\n","w2v_path = os.path.join(path_prefix, 'w2v_all.model')\n","\n","\n","sen_len = 30\n","fix_embedding = True # fix embedding during training\n","batch_size = 32\n","epoch = 20\n","lr = 0.0005#0.001\n","model_dir = path_prefix\n","\n","print(\"loading data ...\")\n","train_x, y = load_training_data(train_with_label)\n","train_x_no_label = load_training_data(train_no_label)\n","\n","# Preprocessing\n","preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\n","embedding = preprocess.make_embedding(load=True)\n","train_x = preprocess.sentence_word2idx()\n","y = preprocess.labels_to_tensor(y)\n","\n","\n","model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=250, num_layers=1, dropout=0.5, fix_embedding=fix_embedding) \n","model = model.to(device) # device為\"cuda\"，model使用GPU來訓練(餵進去的inputs也需要是cuda tensor)\n","\n","X_train, X_val, y_train, y_val = train_x[:130000], train_x[130000:], y[:130000], y[130000:]\n","\n","train_dataset = TwitterDataset(X=X_train, y=y_train)\n","val_dataset = TwitterDataset(X=X_val, y=y_val)\n","\n","# transfor data into batch of tensors\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = True,\n","                                            num_workers = 2)\n","\n","val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = False,\n","                                            num_workers = 2)\n","\n","# Begin Training\n","training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"]},{"cell_type":"code","execution_count":null,"id":"0c7d143b","metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"markdown","id":"9cafaa3b","metadata":{},"source":["## Predict and save to csv file"]},{"cell_type":"code","execution_count":null,"id":"a2cf5b37","metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":73,"id":"56854340","metadata":{"execution":{"iopub.execute_input":"2023-03-24T04:45:14.371980Z","iopub.status.busy":"2023-03-24T04:45:14.371648Z","iopub.status.idle":"2023-03-24T04:45:20.723128Z","shell.execute_reply":"2023-03-24T04:45:20.722044Z","shell.execute_reply.started":"2023-03-24T04:45:14.371948Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["loading testing data ...\n","Get embedding ...\n","loading word to vec model ...\n","get words #23000\n","total words: 23002\n","sentence count #49800\n","load model ...\n","save csv ...\n","Finish Predicting\n"]}],"source":["print(\"loading testing data ...\")\n","test_x = load_testing_data(testing_data)\n","preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\n","embedding = preprocess.make_embedding(load=True)\n","test_x = preprocess.sentence_word2idx()\n","test_dataset = TwitterDataset(X=test_x, y=None)\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n","                                            batch_size = batch_size,\n","                                            shuffle = False,\n","                                            num_workers = 2)\n","print('\\nload model ...')\n","model = torch.load(os.path.join(model_dir, 'ckpt.model'))\n","outputs = testing(batch_size, test_loader, model, device)\n","\n","# save as csv\n","tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"labels\":outputs})\n","print(\"save csv ...\")\n","tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\n","print(\"Finish Predicting\")"]},{"cell_type":"markdown","id":"154fb5c9","metadata":{},"source":["# Hint\n","* Optimizer\n","* learning rate\n","* epoch\n","* batch size\n","* Activation function\n","* Self-Training for unlabel training data"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":5}
